<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
<title>Electric Grid Stability</title>
<meta http-equiv="content-type" content="text/html; charset=iso-8859-1">

<link rel="stylesheet" type="text/css" href="../src/egrid.css">

<link rel="stylesheet" type="text/css" href="../src/index.css">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
</script> 

</head>
<body>
<div id="container">
  <div id="header">
    <h1>Monitoring Electric Grid Stability with Variational Autoencoder
    </h1>
  </div>
  <div id="wrapper">
    <div id="content">
      <a name="top"></a>
      <div id="overview">
        <h2>Overview</h2>
        <ol>
            <li><a href="#section" class="ref-link">Introduction</a></li>
            <li><a href="#section2" class="ref-link"class="ref-link">Method Used</a></li>
            <li><a href="#section3" class="ref-link">Method Discussed in Detail</a></li>
            <li><a href="#section4" class="ref-link">Results</a></li>
        </ol>
      </div>
      <h3 id="section">Introduction</h3>
      <p>This project is inspired by the paper: <a class="link" href="https://www.sciencedirect.com/science/article/abs/pii/S0378779620300602">Unsupervised Feature Learning for Online Voltage Stability Evaluation and Monitoring based on Variational Autoencoder (VAE)</a>. In their paper, Yang et al explored the potential of a probabilistic learning approach for online monitoring of voltage stability. They trained a Variational Autoencoder on Phasor Measurement Unit (PMU) monitoring data. The effectiveness and accuracy of their approach was then compared with other approaches via simulation. The simulation was basically based on power systems with different load increment models. </p>
      <p>Their goal was to capture crucial features that appropriately model the load changing trend as it is a significant factor in voltage stability. However the paper suggests making direct approximation to model the load changing trend is complicated. So they decided to learn low-dimensional features from voltage measurements. This data manifold learning approach they believed could appropriately capture the unknown load profile. Hence, the unknown load profile is treated as a latent variable. The idea of latent variable is, in a way, to represent a data generation process. In other words all observed variables have a "common cause" which is implicitly represented by the latent variable. Unsupervised probabilistic approximation with latent variables usually require methods including Expectation Maximisation, Markov Chain Monte Carlo and Variational Bayes. However, Yang et al indicated that modelling the relation involving nonlinear power equation flow is complex; hence applying the aforementioned methods is computationally intractable. Hence they use deep learning approach analogous to Variational Bayes, i.e. Variational Autoencoder.</p>
      <h3 id="section2">Method Used</h3>
      <p>In this project, I adopt their method for approximating latent variable via VAE to monitor electric voltage stability using <a class="link" href="https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+">Electrical Grid Stability Simulated Data Set dataset</a> from <a class="link" href="https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+">UCI Machine Learning Repository</a>. The data consists of 14 features with 10000 training examples. Each training example is labelled as stable or unstable. However, in training, I only used the stable examples. The method is simply to learn the latent variable for the stable features and regenerate the probability distribution of the stable features. Hence, applying the model parameters to stable input signal outside the training data, should produce closely related distribution and unstable signal yielding negative results.</p>
      <p>Since we are dealing with structured data, the network architecture used only fully connected layers. The network details and architecture is as follows: 
      <ul>
        <li><strong>Size of features:</strong> 2172 x 13 train set and 724 x 13 validation set</li>
        <li><strong>Number of Encoder layers:</strong> 3 with 2 representing statistics from sample distribution</li>
        <li><strong>Number of Decoder layers:</strong> 2</li>
        <li><strong>Input Layer output dimension:</strong> 64</li>
        <li><strong>Output Dimension of Latent Variable:</strong> 32</li>
        <li><strong>Optimizer:</strong>  adam with 1x10-4 learning rate</li>
        <li><strong>Batch Size:</strong>  32</li>
      </ul>
        The model was trained for 20 epochs, evaluating the model on the validation set by computing the loss for every epoch.</p>
      <h3 id="section3">Method Discussed in Detail</h3>
      <p>The idea of VAE is basically to optimize:

		$$\mathcal{L} (\theta,\phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)&emsp; +$$
		\[\qquad \qquad E_{q_\phi(z|x^{(i)})}[\log p_\theta(x^{(i)}|z)]\]    
		
		  w.r.t  \(\theta\) and \(\phi\), where;
		  <ul> 
		   <li>x = observed variable</li>
		   <li>z = latent variable</li>
		   <li>\(\theta\) = variational parameter</li>
		   <li>\(\phi\) = generative parameter</li>
		   <li>\(\mathcal{L}\) = Evidence Lower Bound (<strong>ELBO</strong>) </li>
		   <li>\(D_{KL}\) = Kullback-Leibler Divergence</li>
		   <li>\(p_\theta\) = distribution defined over the observed variables</li>
		   <li>\(q_\phi\) = distribution defined over the latent variables</li> 
		  </ul>
	  </p>
      <p>However due to inconvenience in computing gradient w.r.t \(\phi\), the Monte Carlo gradient estimator, which is an alternative, is in effect impractical. Hence, the approach used in VAE is to reparameterize \(q_\phi\) (z|x) with a differentiable transformation \(g_\phi(\epsilon, x)\), where \(\epsilon\) is an auxiliary noise variable.The Monte Carlo gradient estimator is given by:
        $$\nabla_\phi E_{q_\phi} [f(z)] \simeq \frac{1}{L} \sum _{l=1} ^{L} f(z) \nabla_{q_\phi z^{(l)}}  \log q_\phi (z^{(l)})$$

        where    \(z^{(l)}  \sim q_\phi (z|x^{(i)})\).
        Applying the reparameterized form yields:
        $$ E_{q_\phi(z|x^{(i)})} [f(z)] \simeq \frac{1}{L} \sum _{l=1} ^{L} f(g_\phi (\epsilon^{(l)} ,x^{(i)})) $$
        where    \(\epsilon^{(l)} \sim p(\epsilon)\).
        Hence the evidence lower bound is given by:
        $$\mathcal{L}(\theta,\phi; x^{(i)}) = -D_{KL}(q_\phi(z|x^{(i)})||p_\theta(z)&emsp; + $$ 
       \[\qquad \qquad \frac{1}{L} \sum (\log p_\phi (x^{(l)}|z^{(i,l)}))\]
        where \(z^{(i,l)} \sim g_\phi (\epsilon^{(i,l)}|x^{(i)})\) and \(\epsilon^{(l)} \sim  p(\epsilon)\).</p> 
      <p><b>In summary</b>, VAE just like vanilla Autoencoder comprise the encoder step during which features are downsampled and decoder step in which the features are upsampled to regenerate features drawn from the original data generating process. However, VAE, as opposed to vanilla Autoencoders, involves drawing samples from the distribution of the downsampled data for the decoding process. Hence the encoder step involves a generative process that takes latent variables as input and outputs the parameters for a conditional distribution of the observed variable. Whereas the decoder step involves an inference process which takes an observed variable as input and outputs parameters that define the conditional distribution of the latent profile.<br>
        Encoder step: 
            $$ Generative = p(x|z) $$
        Decoder Step:
            $$ Inference = p(z|x) $$
      Training the VAE involves maximizing the <strong>ELBO</strong> on the marginal log-likelihood:
        $$ \log(p(x)) = \log(\sum_zp(x,z)) = \mathcal{L}(q)+D_{KL} (q||p) $$</p>
      <p>It is observed that maximizing the <strong>ELBO</strong> \((\mathcal{L}(q))\), monotonically increases the log-likelihood of the observed data and in effect decreases the Kullback-Leibler Divergence \((D_{KL} (q||p))\) which in simple terms measures the difference between distribution of the "actual" or reference data generating process and that of the model. 

	  The optimization process is then carried out using sample Monte Carlo estimate of the expectation:

	  $$ \log p(x|z)+\log p(z) - \log q(z|x) $$

	   where z is sampled from \(q(z|x)\) (reparameterization trick).

      <p>With regards to monitoring voltage stability, the lower bound is computed on test input variables not used in the training process using the learned model parameters. The results is then compared with the outputs obtained from the validation dataset used in training. By so doing every input with a lower bound close to that obtained from the model is considered stable and unstable otherwise.</p></p>
      <h3 id="section4">Results</h3>
      <p>In the experiment, test samples were taken from same dataset but were not used for training. 
      The lower bound was computed for both inputs known to be stable and inputs known to be unstable (see figure below). The values computed on the stable signal yielded results close to the model outputs on the validation set. However, the difference between the unstable sample and the model results was very huge; which indicates instability in the network.</p>

      <div class="im-wrapper">
        <div class="im-box">
          <div class="img-format"><img src="../src/img/cnew.png" alt="EGRID"></div>
        </div>
        <p>Lower bound values computed on <span class="unstable">unstable inputs</span>, indicated in <span class="unstable">red</span>, show how they are unrelated to the <span class="model">model outputs</span> (<span class="model">purple</span>). In contrast, the values from the <span class="stable">stable inputs</span>, shown in <span class="stable">blue</span>, are close to the model outputs.</p>
      </div>
      <a href="#top" id="back-to-top"><p>&#8593 Back to Top</p></a>
    </div> 
  </div>

  <div id="navigation" class="bar">
  
    <ul>
      <li class="centre"><a href="index.html"><p class="ref-link size"><strong>Benjamin Essilfie-Nyame</strong></p></a></li>
      <li class="centre"><a href="mailto:benessilfie60@yahoo.com" class="fa fa-yahoo"></a>
        <a href="https://github.com/nyameben" class="fa fa-github"></a>
      </li>
    </ul>

  </div>
  <div id="extra">
    <h2>References</h2>

    <h3>Papers</h3>
      <ul>
        <li >
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S0378779620300602"><div class="ref-link"> Unsupervised Feature Learning for Online Voltage Stability Evaluation and Monitoring based on Variational Autoencoder</div></a>
        </li>
        <li>
          <a class="ref-link" href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a>
        </li>
      </ul>
      <h3>Books</h3>
      <ul>
        <li>
          Chapter 20 Deep Generative Models,
          <a class="ref-link" href="https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=as_li_ss_tl?keywords=Deep+Learning&qid=1563148923&s=books&sr=1-1&linkCode=sl1&tag=inspiredalgor-20&linkId=6e39ca521cd3c37b4a192f0ad8cfcf8c&language=en_US">Deep Learning</a>,  2016.
        </li>
        <li>
          Chapter 10 Approximate Inference,
          <a class="ref-link" href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=as_li_ss_tl?keywords=Pattern+Recognition+and+Machine+Learning&qid=1563148939&s=books&sr=1-1&linkCode=sl1&tag=inspiredalgor-20&linkId=d655f6e30d43b3675f6389f04db17f64&language=en_US"> Pattern Recognition and Machine Learning</a>, 2006.
        </li>
        <li>
          Chapter 21 Variational Inference,
          <a class="ref-link" href="https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020"> Machine Learning - A Probabilistic Perspective</a>, 2012.
        </li>
      </ul>
      <h3>API / Framework</h3>

      <ul>
        <li>
          <a class="ref-link" href="https://www.tensorflow.org/tutorials/generative/cvae">Tensorflow tutorial on Convolutional Variational Autoencoder </a>
        </li>
      </ul>
  </div>
  <div id="footer">
    <p>&#169; 2020 Benjamin Essilfie-Nyame.</p>
  </div>
</div>
</body>
</html>
